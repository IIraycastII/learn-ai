1. Data simulation

You generate 100 random x and y values between 80 and 100.
They represent two input features for a simple neuron.

2. Weighted sum (the neuronâ€™s input)

You compute:

ğ‘§=ğ‘¤1ğ‘¥+ğ‘¤2ğ‘¦+ğ‘
z=w1â€‹x+w2y+b

Here both weights w1 and w2 are set to 1, and bias b = 0, so:

ğ‘§=ğ‘¥+ğ‘¦z=x+y

This gives a combined activation input for each data pair.

3. Scaling (normalization)

The raw z values can be large (e.g., around 160â€“200 since x and y â‰ˆ 80â€“100).
To make the sigmoid curve meaningful, you normalize all z values into the range [-10, 10], which is the region where the sigmoid changes most sharply:

ğ‘§scaled=(ğ‘§âˆ’ğ‘§minâ¡)(ğ‘§maxâ¡âˆ’ğ‘§min)Ã—20âˆ’10z
scaled=(zmaxâˆ’zmin)(zâˆ’zmin)Ã—20âˆ’10

This step ensures your sigmoid plot shows the classic â€œSâ€ shape.

4. Applying the sigmoid function

You then compute:

ğœ(ğ‘§)=11+ğ‘’âˆ’ğ‘§
Ïƒ(z)=1+eâˆ’z1

for each scaled z value.
This converts each input into a value between 0 and 1, which is the neuron's activation output â€” the probability-like interpretation.

5. Plot 1 â€“ Sigmoid curve

You sort the scaled z values and plot Ïƒ(z) vs. z, giving you the classic S-shaped sigmoid curve.

The red dashed line at 0.5 represents the decision threshold â€” the point where the neuron output switches from "off" (0) to "on" (1).

6. Plot 2 â€“ Original data visualization

You also display a scatter plot of the original (x, y) points â€” purely for reference, showing the random data that fed into the neuronâ€™s weighted sum.
