1. Dataset Setup

You start with:

Study Hours	Attendance	Results
2	60	fail
4	75	pass
1	40	fail
5	80	pass

You convert â€œpassâ€ â†’ 1 and â€œfailâ€ â†’ 0, so the neural model can process numbers.

2. Model Initialization

You define:

weights_1 = 0.1 â†’ for study hours

weights_2 = -0.3 â†’ for attendance

bias = -0.2

learning_rate = 0.1

This model is a single-layer perceptron:
ğ‘§=(ğ‘¤1Ã—ğ‘¥1)+(ğ‘¤2Ã—ğ‘¥2)+ğ‘
z=(w1Ã—x1)+(w2Ã—x2)+b

And the output rule is:

if ğ‘§â‰¥0.3â‡’predict 1(pass),else 0(fail)
if zâ‰¥0.3â‡’predict 1 (pass), else 0 (fail)
3. Data Normalization

Before training, you normalize both input features between 0 and 1:
ğ‘¥â€²=ğ‘¥âˆ’min(ğ‘¥)max(ğ‘¥)âˆ’min(ğ‘¥)
xâ€²=max(x)âˆ’min(x)xâˆ’min(x)
	â€‹


This ensures features are in the same scale â€” a key step for gradient-based learning.

4. Learning Loop

You run the perceptron for multiple iterations (input_times).

For each training example:

Compute the weighted sum:

ğ‘§=ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+ğ‘
z=w1x1+w2x2+b

Apply the threshold to get the predicted output (0 or 1).

Compare it to the actual output (input_3).

If prediction is wrong, update the weights:

ğ‘¤ğ‘–=ğ‘¤ğ‘–+ğœ‚Ã—(ğ‘¦âˆ’ğ‘¦^)Ã—ğ‘¥ğ‘–
wi=wi+Î·Ã—(yâˆ’y^â€‹)Ã—xi
ğ‘=ğ‘+ğœ‚Ã—(ğ‘¦âˆ’ğ‘¦^)
b=b+Î·Ã—(yâˆ’y^)

where:
Î· = learning rate
y = actual result
y^= predicted result

This update rule is the core perceptron learning algorithm.

5. Accuracy Tracking

For every correct prediction, you increase correct_count.
At the end, you print how many predictions were correct across all iterations.
